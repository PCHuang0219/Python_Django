{% extends 'wiki_template.html' %}
{% load staticfiles %}

{% block titile %}
<title>TMS-Wiki</title>
{% endblock %}
{% block style %}
	<!-- Additional CSS-->
	<link href="{% static 'vendors/additional/product.css' %}" rel="stylesheet">
	<link href="{% static 'vendors/additional/wiki.css' %}" rel="stylesheet">
{% endblock %}

{% block body %}
	<div class="tabcontent" id="Article">
		<h3>Data center network architectures</h3>
		<p class="font1">From Wikipedia, the free encyclopedia<br>
		<br>
		Data center is a pool of resources (computational, storage, network) interconnected using a communication network.[1][2] Data Center Network (DCN) holds a pivotal role in a data center, as it interconnects all of the data center resources together. DCNs need to be scalable and efficient to connect tens or even hundreds of thousands of servers to handle the growing demands of Cloud computing.[3][4] Today’s data centers are constrained by the interconnection network.[5]</p>
		<div class="contents">
			<ul class="ul1">
				<li style="list-style: none"><b>Contents</b></li>
				<li>Types of Data center network</li>
				<li style="list-style: none; display: inline">
					<ul class="ul2">
						<li>1.1 Three-tier DCN</li>
						<li>1.2 Fat tree DCN</li>
						<li>1.3 DCell</li>
						<li>1.4 Others</li>
					</ul>
				</li>
				<li>Challenges</li>
				<li>Performance Analysis of DCNs</li>
				<li>Structural robustness and Connectivity of DCNs</li>
				<li>Energy efficiency of DCNs</li>
				<li>References</li>
			</ul>
		</div>
		<h3>Types of Data center network</h3>
		<p class="font1"><b class="font2">Three-tier DCN</b><br>
		The legacy three-tier DCN architecture follows a multi-rooted tree based network topology composed of three layers of network switches, namely access, aggregate, and core layers.[6] The servers in the lowest layers are connected directly to one of the edge layer switches. The aggregate layer switches interconnect together multiple access layer switches. All of the aggregate layer switches are connected to each other by core layer switches. Core layer switches are also responsible for connecting the data center to the Internet. The three-tier is the common network architecture used in data centers.[6] However, three-tier architecture is unable to handle the growing demand of cloud computing.[7] The higher layers of the three-tier DCN are highly oversubscribed.[3] Moreover, scalability is another major issue in three-tier DCN. Major problems faced by the three-tier architecture include, scalability, fault tolerance, energy efficiency, and cross-sectional bandwidth. The three-tier architecture uses enterprise-level network devices at the higher layers of topology that are very expensive and power hungry.[5]<br>
		<b class="font2">Fat tree DCN</b><br>
		Fat tree DCN architecture handles the oversubscription and cross section bandwidth problem faced by the legacy three-tier DCN architecture. Fat tree DCN employs commodity network switches based architecture using Clos topology.[3] The network elements in fat tree topology also follows hierarchical organization of network switches in access, aggregate, and core layers. However, the number of network switches is much larger than the three-tier DCN. The architecture is composed of k pods, where each pod contains, (k/2)2 servers, k/2 access layer switches, and k/2 aggregate layer switches in the topology. The core layers contain (k/2)2 core switches where each of the core switches is connected to one aggregate layer switch in each of the pods. The fat tree topology offers 1:1 oversubscription ratio and full bisection bandwidth.[3] The fat tree architecture uses a customized addressing scheme and routing algorithm. The scalability is one of the major issues in fat tree DCN architecture and maximum number of pods is equal to the number of ports in each switch.[7]<br>
		<b class="font2">DCell</b><br>
		DCell is a server-centric hybrid DCN architecture where one server is directly connected to many other servers.[4] A server in the DCell architecture is equipped with multiple Network Interface Cards (NICs). The DCell follows a recursively built hierarchy of cells. A cell0 is the basic unit and building block of DCell topology arranged in multiple levels, where a higher level cell contains multiple lower layer cells. The cell0 is building block of DCell topology, which contains n servers and one commodity network switch. The network switch is only used to connect the server within a cell0. A cell1 contain k=n+1 cell0 cells, and similarly a cell2 contains k * n + 1 dcell1. The DCell is a highly scalable architecture where a four level DCell with only six servers in cell0 can accommodate around 3.26 million servers. Besides very high scalability, the DCell architecture depicts very high structural robustness.[8] However, cross section bandwidth and network latency is a major issue in DCell DCN architecture.[1]<br></p>
		<h3>Chanllenges</h3>
		<p class="font1">Scalability is one of the foremost challenges to the DCNs.[3] With the advent of cloud paradigm, data centers are required to scale up to hundreds of thousands of nodes. Besides offering immense scalability, the DCNs are also required to deliver high cross-section bandwidth. Current DCN architectures, such as three-tier DCN offer poor cross-section bandwidth and possess very high over-subscription ratio near the root.[3] Fat tree DCN architecture delivers 1:1 oversubscription ratio and high cross section bandwidth, but it suffers from low scalability limited to k=total number of ports in a switch. DCell offers immense scalability, but it delivers very poor performance under heavy network load and one-to-many traffic patterns.</p>
		<h3>References</h3>
		<ul class="ul1">
			<li>K. Bilal, S. U. Khan, L. Zhang, H. Li, K. Hayat, S. A. Madani, N. Min-Allah, L. Wang, D. Chen, M. Iqbal, C.-Z. Xu, and A. Y. Zomaya, "Quantitative Comparisons of the State of the Art Data Center Architectures," Concurrency and Computation: Practice and Experience, vol. 25, no. 12, pp. 1771-1783, 2013.</li>
			<li>M. Noormohammadpour, C. S. Raghavendra, "Datacenter Traffic Control: Understanding Techniques and Trade-offs," IEEE Communications Surveys & Tutorials, vol. PP, no. 99, pp. 1-1.</li>
			<li>M. Al-Fares, A. Loukissas, A. Vahdat, A scalable, commodity data center 2 network architecture, in: ACM SIGCOMM 2008 Conference on Data 3 Communication, Seattle,WA, 2008, pp. 63–74.</li>
		</ul>
	</div>
	<div class="tabcontent" id="Talk">
		<h3>News</h3>
		<p>Some news this fine day!</p>
	</div>
	<div class="tabcontent" id="Read">
		<h3>Contact</h3>
		<p>Get in touch, or swing by for a cup of coffee.</p>
	</div>
	<div class="tabcontent" id="About">
		<h3>About</h3>
		<p>Who we are and what we do.</p>
	</div>
	<div class="tabcontent" id="MainPage">
		<h3>What is SONiC ?</h3>
		<p class="font1">Software for Open Networking in the Cloud (SONiC), is a breakthrough for network switch operations and management. Edge-corE Networks joined as a partner that provided multiple switching platforms to accelerate this growing ecosystem of SONiC. To release the flexibility and extend the degree-of-freedom, the cloud community can cherry pick best solutions with Edge-corE 10G, 25G, 40G, 100G or 400G featured switch hardware.<br>
		<br>
		Edge-CorE 10G - AS5512-54x, AS57112-54x, AS5812-54t, AS5812-54x<br>
		Edge-CorE 25G - AS7312-54xs, AS7326-56x<br>
		Edge-CorE 40G - AS6700-32x, AS6701-32x, AS6712-32x, AS6812-32x<br>
		Edge-CorE 100G - <a href="/sonic/products/100g/as7712-32x">AS7712-32x</a>, AS7726-32x, AS7816-64x, WEDGE100bf-32x, WEDGE100bf-65x, WEDGE100s-32x<br>
		Edge-CorE 400G - AS8000, AS9716-32d</p><br>
		<h3>Increasingly strong momentum</h3>
		<p class="font1">SONiC is fully open sourced and enables industrial collaborators, researchers and innovators to contribute components that will benefit millions of customers. Based on the Switch Abstration Interface (SAI), Network hardware vendors (e.g. Edge-corE, Dell, Facebook, Mellanox, etc.) can consistently keep the programming interface to ASIC while develop the state-of-the-art high speed hardware platforms. This enables one unified software solution to leverage the rapid innovation in silicon, CPU, power, port density, optics and speed across multiple platforms.</p>
		<h3>Faster software evolution</h3>
		<p class="font1">With multiple containerized components, you can upgrade the flawed container with the new code, plug in new components, third-party, proprietary or open sourced software with minimum effort, instead of replacing the entire switch image. SONiC realizes fine-grained failure recovery and enables in-service upgrades with zero downtime. Operators can specify their specific scenarios at cloud networking with the aid of SONiC.<br>
		<br>
		For more information, please visit the SONiC github page: https://github.com/Azure/SONiC/wiki</p>
	</div>
{% endblock %}
{% block js %}
	<!-- Additional JS-->
	<script src="{% static 'js/sonic/product.js' %}"></script>
	<script src="{% static 'js/sonic/location.js' %}"></script>
	<script src="{% static 'vendors/additional/js/wiki.js' %}"></script>
{% endblock %} 