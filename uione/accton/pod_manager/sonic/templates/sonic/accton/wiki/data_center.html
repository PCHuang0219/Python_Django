{% extends 'wiki_template.html' %}
{% load staticfiles %}

{% block titile %}
<title>Wiki-Data center network architectures</title>
{% endblock %}
{% block style %}
<!-- Additional CSS-->
<link href="{% static 'vendors/additional/product.css' %}" rel="stylesheet">
<link href="{% static 'vendors/additional/wiki.css' %}" rel="stylesheet">
{% endblock %}

{% block body %}
<div class="tabcontent" id="Article">
	<h3>Data center network architectures: A brief overview</h3>
	<p class="wikifont1">- organized by Alvin Li<br>
		<br>
		The data center network (DCN) plays a central role in the modern data centers - the core infrastructures of
		cloud computing. The efficiency and performance of a data center remarkably rely on the DCN, which connects the
		physical components of data centers, e.g., servers, switches, in a specific topology with cables and optical
		fibers. In this survey, we are going to overview the features, hardware, and architectures of DCN's briefly.
		<br><br>
		In 2006, Google CEO came up with the concept of "cloud computing" in business <a href="#cite note-1">[1]</a>, whilst Amazon introduced the
		Elastic Compute Cloud (Amazon EC2) providing the capacity of resizable computing in the cloud <a href="#cite note-2">[2]</a>. The term -
		"cloud" or "cloud computing", means the provision of dynamically scalable resources as a service to
		simultaneously run a program on many interconnected computers (the Internet)  <a href="#cite note-3">[3]</a>. Having a right browser in your
		mobile phone, you can access to the cloud services. Depending on the level of resources, cloud services can be
		categorized into different modes. Infrastructure-as-a-Service (IaaS) is the most basic one that providers offer
		computers placed in a building known as "a data center". Other modes include Platform-as-a-Service (PaaS),
		Software-as-a-Service (SaaS), Anything-as-a-Service (XaaS), and Data-Center-as-a-Service (DCaaS). These services
		can be acquired by tenants on demand based on service-level agreements (SLAs).
	</p>
	<div class="contents">
		<div class="ul1" style="margin-left: 2em">
			<b>Contents</b>
			<ul>
				<a href="#Production data centers">
					<li>Production data centers</li>
				</a>

				</li>
				<a href="#Hardware of data center networks">
					<li>Hardware of data center networks</li>
				</a>
				<a href="#Architectures of data center networks">
					<li>Architectures of data center networks</li>
				</a>
				<a href="#Switch-centric architectures: Tree-like, Flat, and Unstructured">
					<li>Switch-centric architectures: Tree-like, Flat, and Unstructureds</li>
				</a>
				<a href="#Tree-like switch-centric architectures">
					<li>Tree-like switch-centric architectures</li>
				</a>
				<a href="#Flat switch-centric architectures">
					<li>Flat switch-centric architectures</li>
				</a>
				<a href="#Unstructured switch-centric architectures">
					<li>Unstructured switch-centric architectures</li>
				</a>
				<a href="#Performance Analysis of DCNs">
					<li>Performance Analysis of DCNs</li>
				</a>
				<a href="#Structural robustness and Connectivity of DCNs">
					<li>Structural robustness and Connectivity of DCNs</li>
				</a>
				<a href="#Energy efficiency of DCNs">
					<li>Energy efficiency of DCNs</li>
				</a>
				<a href="#References">
					<li>References</li>
				</a>
			</ul></a>
		</div>
	</div>
	<h3 id="Production data centers">Production data centers</h3>

	<p class="wikifont1">The main features of data centers (DC's) include size, infrastructure tiers, and modularity
		(one measure of the structure of networks). For large IT companies, production data centers are indispensable.
		Representative production data centers like Portland Dalles Data center, which was cost about $600 million that
		Google built it in 2006. It is a pair of 94,000-square-foot DC's that locate on the banks of Columbia River with
		two four-story cooling towers to lower the water temperature. Another Google's typical DC is Georgia Douglas
		County Data Center providing crucial business services, such as searching, Gmail, and Google Maps. Google owns
		36 production DC's globally, 19 of which are in America, 12 in Europe, 3 in Asia, 1 in Russia and 1 in South
		America. Microsoft owns production DC's also around the globe, it built Washington Quincy Data Center with an
		area of 75 acres in 2007. Other large IT companies, IBM, Amazon, HP, Dell and Apple, etc., also mange and build
		their own production DC's for business services.
		<br><br>
		Production DC's are growing in size dramatically to satisfy the growing demand for cloud services. According to
		the number of racks, DC's can be divided into huge scale ( > 10,000 racks), large scale (3000 - 10,000 racks),
		and median and small scale ( < 3000 racks). <br><br>
			"Tier Classifications Define Site Infrastructure Performance" <a href="#cite note-3">[4]</a> -
			DC's can be classified into four tiers. Tier requirements and common attributes are summarized in the below
			table <a href="#cite note-5">[5]</a>.</p>
	<img src="{%static '/images/wiki/image001.png'%}" style="width:100%">

	<h3 id="Hardware of data center networks">Hardware of data center networks</h3>
	<p class="wikifont1">Switches are the backbone of many DCN architectures. For example, fat-tree <a href="#cite note-6">[6]</a> and VL2 <a href="#cite note-7">[7]</a> are
		both three-layer network architecture, including core switch layer, aggregation switch layer, and edge switch
		(Top of Rack, ToR) layer. Servers are the core physical components of DCN architectures, which process, analyze,
		store, and transmit massive data and directly determine the performance of DCN's. Servers can be categorized to
		three types according to the appearance shape: tower servers, rack servers, and blade servers. Especially, rack
		servers are the mainstream servers used today. A rack can contain several servers arranged like drawers with
		standard space-saving and maintainable properties.The cons of it are poor heat dissipation and high cabling
		complexity due to dense placements. The storage systems of DC's have two main types in convention: one is
		Network-attached storage (NAS) and another one is Storage area network (SAN). NAS is file-oriented storage
		network, providing storage access for a heterogeneous group of computer systems; SAN is a high-speed storage
		network, providing enhanced access to consolidated, block-level or file-level data for servers. However, as the
		volume of data in DC's and unstructured data (e.g., video, photo, and voice) are growing faster than ever before
		in recent years, the centralized management In a tree-like switch-centric architecture, the switches are
		interconnected to form a multi-rooted tree.n SAN/NAS may not be adaptive in cloud DC's. A distributed and
		effective management for mass storage is required to support better cloud services, such as Windows Azure
		Storage [8[ and Amazon Simple Storage Service (Amazon S3) <a href="#cite note-9">[9]</a>. The other new trend cloud like Software Defined
		Storage (SDS) may be applied to meet the requirements specified through the service management interface <a href="#cite note-10">[10]</a>.
		Racks and cables are essential to DCN architectures to support easy management and space saving, interconnecting
		the other components (switches, servers, and storage devices), respectively. </p>
	<h3 id="Architectures of data center networks">Architectures of data center networks</h3>
	<p class="wikifont1">A traditional DCN has a three-layer, mutli-rooted tree-like architecture as the figure shown
		below:<br><br>
	<img src="{%static '/images/wiki/image002.png'%}" style="width:80%">
		<br>
		With the dramatic increase in tenants, DCN's must be able to interconnect hundreds of thousands or even millions
		of servers and provide sufficient bandwidth to ensure the quality of cloud services, and also need to be
		flexible, reliable, and have high density to ensure that the various applications run steadily and efficiently.
		However, the traditional DCN's have met several challenges since it is not designed for cloud data centers. The
		inherent disadvantages include:
		<br><br>
		1) Limited bandwidth.<br>
		The bandwidth of a server is really limited. When the workloads reach the peak, the core switched may become
		bottlenecks and can make it at the risk of being in a crash.<br>
		2) Poor flexibility.<br>
		The port number of core switches determines the maximum number of servers supported in the multi-rooted
		tree-like architectures.<br>
		3) Low utilization.<br>
		The conventional DCN's are generally divided into multiple domains in layer 2 to ensure security and
		manage-ability. This set-up results in massive fragmentation of resources, which are not suitable for
		large-scale cloud computing.<br>
		4) Complex cabling.<br>
		Once the scale of the traditional DCN expand to a large size, the number of cables can be enormous, such that
		cabling becomes a heavy and complex task as servers increasing.<br>
		5) High cost.<br>
		The switches in the core and aggregation layers are usually enterprise-level switches that are very expensive
		and power hungry, resulting in higher Capital Expenditure (CAPEX) and Operation Expense (OPEX).
		<br><br>
		The modern DCN’s should avoid the disadvantages of traditional DCN’s and have full bandwidth, good scalability,
		high utilization, easy cabling, and low cost to provide high-quality cloud services.

	</p>
	<h3 id="Switch-centric architectures: Tree-like, Flat, and Unstructured">Switch-centric architectures: Tree-like,
		ㄒFlat, and Unstructured</h3>
	<p class="wikifont1">The switches are enhanced to accommodate networking and routing requirements, whereas the
		servers are almost all unmodified. Switch-centric architectures can be divided into tree-like, flat, and
		unstructured architectures according to the structural properties. </p>
	<h3 id="Tree-like switch-centric architectures">Tree-like switch-centric architectures</h3>
	<p class="wikifont1">In a tree-like switch-centric architecture, the switches are interconnected to form a
		multi-rooted tree.</p>
	<h3 id="Flat switch-centric architectures">Flat switch-centric architectures</h3>
	<p class="wikifont1">Flat switch-centric architectures flatten the three or more switch layers down to two or only a
		single switch layer, which simplify the management and maintenance of DCN's.</p>

	<h3 id="Unstructured switch-centric architectures">Unstructured switch-centric architectures</h3>
	<p class="wikifont1">Unstructured switch-centric architectures are irregular or asymmetric architectures that are
		feasible for DCN's. Solutions for addressing, routing, and load balancing were presented in arbitrary networks
		in recent years <a href="#cite note-11">[11-13]</a>.
	</p>
	<h3 id="Performance Analysis of DCNs">Performance Analysis of DCNs</h3>
	<p class="wikifont1">A quantitative analysis of the three-tier, fat tree, and DCell architectures for
		performance
		comparison (based on throughput and latency) is performed for different network traffic pattern.The
		fat tree
		DCN delivers high throughput and low latency as compared to three-tier and DCell. DCell suffers from
		very low
		throughput under high network load and one to many traffic patterns. One of the major reasons for
		DCell’s low
		throughput is very high over subscription ratio on the links that interconnect the highest level
		cells.</p>
	<h3 id="Structural robustness and Connectivity of DCNs">Structural robustness and Connectivity of DCNs
	</h3>
	<p class="wikifont1">The DCell exhibits very high robustness against random and targeted attacks and
		retains
		most of its node in the giant cluster after even 10% of targeted failure. multiple failures
		whether
		targeted or random, as compared to the fat tree and three-tier DCNs.One of the major reasons
		for high
		robustness and connectivity of the DCell is its multiple connectivity to other nodes that is not
		found in
		fat tree or three-tier architectures.</p>
	<h3 id="Energy efficiency of DCNs">Energy efficiency of DCNs</h3>
	<p class="wikifont1">The concerns about the energy needs and environmental impacts of data centers are
		intensifying. Energy efficiency is one of the major challenges of today’s Information and
		communications
		technology (ICT) sector. The networking portion of a data center is accounted to consume around 15%
		of
		overall cyber energy usage. Around 15.6 billion kWh of energy was utilized solely by the networks
		infrastructure within the data centers worldwide . The energy consumption by the network
		infrastructure
		within a data center is expected to increase to around 50% in data centers.IEEE 802.3az standard
		has
		been standardized in 2011 that make use of adaptive link rate technique for energy efficiency.
		Moreover,
		fat tree and DCell architectures use commodity network equipment that is inherently energy
		efficient.
		Workload consolidation is also used for energy efficiency by consolidating the workload on few
		devices to
		power-off or sleep the idle devices.</p>
	<h3 id="References">References</h3>
	<ul class="ul1">
		<li id="cite note-1">Amazon, Announcing Amazon Elastic Compute Cloud, Amazon EC2—beta, 2006.
			<a
				href="https://aws.amazon.com/cn/about-aws/whats-new/2006/08/24/announcing-amazon-elastic-compute-cloud-amazon-ec2---beta/">https://aws.amazon.com/cn/about-aws/whats-new/2006/08/24/announcing-amazon-elastic-compute-cloud-amazon-ec2---beta/</a>
		</li>
		<li id="cite note-2"> E. Schmidt, A Conversation With Google CEO Eric Schmidt, 2006.
			<a
				href=". http://www.google.com/press/podium/ses2006.html">http://www.google.com/press/podium/ses2006.html</a>
		</li>
		<li id="cite note-3">Wikipedia, Cloud computing, 2013
			<a href="http://en.wikipedia.org/wiki/Cloud_computing">http://en.wikipedia.org/wiki/Cloud_computing</a>
		</li>
		<li id="cite note-4">W. Turner, J. Seader, K. Brill, Industry standard tier classifications define site
			infrastructure performance, white paper Tech. Rep., The Uptime Institute, 2005.</li>
		<li id="cite note-5"><a
				hef="http://dx.doi.org/10.1016/j.jpdc.2016.05.009">http://dx.doi.org/10.1016/j.jpdc.2016.05.009</a></li>
		<li id="cite note-6">M. Al-Fares, A. Loukissas, A. Vahdat, A scalable, commodity data center network
			architecture, ACM SIGCOMM Comput. Commun. Rev. 38 (4) (2008) 63–74.

		</li>
		<li id="cite note-7">A. Greenberg, J.R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D.A. Maltz, P. Patel,
			S. Sengupta, VL2: a scalable and flexible data center network, ACM SIGCOMM Comput. Commun. Rev. 39 (4)
			(2009) 51–62.</li>
		<li id="cite note-8">
				B. Calder, J. Wang, A. Ogus, N. Nilakantan, A. Skjolsvold, S. McKelvie, Y.
				Xu, S. Srivastav, J. Wu, H. Simitci, et al., Windows Azure Storage: a highly
				available cloud storage service with strong consistency, in: Proceedings of
				the Twenty-Third ACM Symposium on Operating Systems Principles, ACM,
				2011, pp. 143–157.
				</li>
		<li id="cite note-9">Amazon, Amazon S3, 2016<a href="http://aws.amazon.com/s3/">http://aws.amazon.com/s3/</a></li>
		<li id="cite note-10">Storage Networking Industry Association, Software Defined Storage, 2016.
		<a href="http://www.snia.org/sds">http://www.snia.org/sds</a>	
		</li>
		<li id="cite note-11">
				C. Kim, M. Caesar, J. Rexford, Floodless in seattle: a scalable ethernet
				architecture for large enterprises, ACM SIGCOMM Comput. Commun. Rev. 38
				(4) (2008) 3–14.
				</li>
		<li id="cite note-12">
				J. Touch, R. Perlman, Transparent interconnection of lots of links (TRILL):
				Problem and applicability statement, Tech. Rep., RFC 5556, Internet
				Engineering Task Force, 2009.
				</li>
		<li id="cite note-13">
				J. Mudigonda, P. Yalagandula, M. Al-Fares, J.C. Mogul, SPAIN: COTS data-
				center ethernet for multipathing over arbitrary topologies, in: USENIX NSDI,
				2010, pp. 265–280.
				</li>
		
	</ul>
</div>
{% endblock %}
{% block js %}
<!-- Additional JS-->
<script src="{% static 'js/sonic/product.js' %}"></script>
<script src="{% static 'js/sonic/location.js' %}"></script>
<script src="{% static 'vendors/additional/js/wiki.js' %}"></script>
{% endblock %}